#!/usr/bin/env python3
# Generates switch table-based helpers for the tokenizer.
# Inputs:
#   - lang/keywords.inc:      lines "<keyword> <TOKEN_ENUM>"
#   - lang/punctuators.inc:   lines "<lexeme> <TOKEN_ENUM>" for 1,2,3-char punctuators
# Outputs:
#   - lang/tokenizer_keywords_switch.inc: classify_identifier_ascii(const char*, s64)
#   - lang/tokenizer_punct_switch.inc:    scan_punct(const char*, s64, s64*) longest-match punctuation
#   - lang/token_print.inc:               token_to_string(token_type)

import os
import sys
from collections import defaultdict

REPO_ROOT = os.path.join(os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir)), 'lang', 'src', 'tokenizer')
KEYWORDS_INC = os.path.join(REPO_ROOT, 'keywords.inc')
OUT_INC = os.path.join(REPO_ROOT, 'tokenizer_keywords_switch.inc')
PUNCTS_INC = os.path.join(REPO_ROOT, 'punctuators.inc')
OUT_PUNCTS = os.path.join(REPO_ROOT, 'tokenizer_punct_switch.inc')
OUT_PRINT = os.path.join(REPO_ROOT, 'token_print.inc')


def parse_keywords(path):
    kws = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#') or line.startswith('//'):
                continue
            parts = line.split()
            if len(parts) != 2:
                raise ValueError(f'Invalid line in keywords.inc: {line!r}')
            word, enum = parts
            kws.append((word, enum))
    return kws


def parse_puncts(path):
    puncts = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#') or line.startswith('//'):
                continue
            parts = line.split()
            if len(parts) != 2:
                raise ValueError(f'Invalid line in punctuators.inc: {line!r}')
            lex, enum = parts
            puncts.append((lex, enum))
    return puncts


def escape_char(ch: str) -> str:
    # Escape for inclusion inside single quotes in C code.
    if ch == "'":
        return "\\'"
    if ch == "\\":
        return "\\\\"
    return ch


def emit_classifier(kws):
    groups = defaultdict(lambda: defaultdict(list))  # len -> first_char -> [(word, enum)]
    for w, e in kws:
        if not w:
            continue
        groups[len(w)][w[0]].append((w, e))

    lines = []
    lines.append('// Generated by tools/gen_tokenizer.py. Do not edit by hand.')
    lines.append('static inline token_type classify_identifier_ascii(const char* s, s64 len) {')
    lines.append('  switch (len) {')
    for length in sorted(groups.keys()):
        lines.append(f'    case {length}: {{')
        lines.append('      switch (s[0]) {')
        for ch in sorted(groups[length].keys()):
            lines.append(f"        case '{escape_char(ch)}': {{")
            for w, e in groups[length][ch]:
                # generate chain of char checks s[1]==... && s[2]==...
                conds = []
                for i, c in enumerate(w[1:], start=1):
                    conds.append(f"s[{i}]=='{escape_char(c)}'")
                if conds:
                    cond = ' && '.join(conds)
                    lines.append(f'          if ({cond}) return {e};')
                else:
                    # single letter keyword
                    lines.append(f'          return {e};')
            lines.append('          break;')
            lines.append('        }')
        lines.append('      default: break;')
        lines.append('      }')
        lines.append('      break;')
        lines.append('    }')
    lines.append('    default: break;')
    lines.append('  }')
    lines.append('  return TOKEN_IDENTIFIER;')
    lines.append('}')
    return '\n'.join(lines) + '\n'


def c_string_literal(s: str) -> str:
    # Escape for inclusion inside C double quotes
    out = '"'
    for ch in s:
        if ch == '"':
            out += '\\"'
        elif ch == '\\':
            out += '\\\\'
        elif ch == '\n':
            out += '\\n'
        elif ch == '\t':
            out += '\\t'
        else:
            out += ch
    out += '"'
    return out


def emit_punct_switch(puncts):
    # Group by first char; for each, order by length desc for longest-match
    groups = {}
    for lex, enum in puncts:
        groups.setdefault(lex[0], []).append((lex, enum))
    for k in groups:
        groups[k].sort(key=lambda x: -len(x[0]))

    lines = []
    lines.append('// Generated by tools/gen_tokenizer.py. Do not edit by hand.')
    lines.append('typedef struct { token_type Type; s64 Length; } scan_punct_result;')
    lines.append('static inline scan_punct_result scan_punct(const char* s, s64 remaining) {')
    lines.append('  scan_punct_result r = { TOKEN_INVALID, 0 };')
    lines.append('  if (remaining <= 0) return r;')
    lines.append('  switch (s[0]) {')
    for ch, arr in sorted(groups.items()):
        lines.append(f"    case '{escape_char(ch)}': {{")
        for lex, enum in arr:
            L = len(lex)
            if L == 1:
                # If nothing else matched, single-char
                lines.append(f'      r.Type = {enum}; r.Length = 1; return r;')
            else:
                cond = [f'remaining >= {L}']
                for i, c in enumerate(lex[1:], start=1):
                    cond.append(f"s[{i}]=='{escape_char(c)}'")
                cond_str = ' && '.join(cond)
                lines.append(f'      if ({cond_str}) {{ r.Type = {enum}; r.Length = {L}; return r; }}')
        lines.append('      break;')
        lines.append('    }')
    lines.append('    default: break;')
    lines.append('  }')
    lines.append('  return r;')
    lines.append('}')
    return '\n'.join(lines) + '\n'


def emit_token_print(keywords, puncts):
    # Build mapping for keywords and puncts, plus some defaults for others
    pairs = []
    for w, e in keywords:
        pairs.append((e, w))
    for lex, e in puncts:
        pairs.append((e, lex))
    # Add a few common non-lexeme tokens
    pairs.extend([
        ('TOKEN_IDENTIFIER', '<identifier>'),
        ('TOKEN_INTEGER', '<int>'),
        ('TOKEN_FLOAT', '<float>'),
        ('TOKEN_STRING_DOUBLE_QUOTE', '"'),
        ('TOKEN_STRING_SINGLE_QUOTE', "'"),
        ('TOKEN_STRING_WIDE_DOUBLE_QUOTE', 'L"'),
        ('TOKEN_STRING_WIDE_SINGLE_QUOTE', "L'"),
    ])

    lines = []
    lines.append('// Generated by tools/gen_tokenizer.py. Do not edit by hand.')
    lines.append('const char* token_to_string(token_type t) {')
    lines.append('  switch (t) {')
    for e, text in pairs:
        lines.append(f'    case {e}: return {c_string_literal(text)};')
    lines.append('    default: return "<unknown>";')
    lines.append('  }')
    lines.append('}')
    return '\n'.join(lines) + '\n'


def main():
    kws = parse_keywords(KEYWORDS_INC)
    puncts = parse_puncts(PUNCTS_INC)
    os.makedirs(os.path.dirname(OUT_INC), exist_ok=True)
    with open(OUT_INC, 'w', encoding='utf-8') as f:
        f.write(emit_classifier(kws))
    with open(OUT_PUNCTS, 'w', encoding='utf-8') as f:
        f.write(emit_punct_switch(puncts))
    with open(OUT_PRINT, 'w', encoding='utf-8') as f:
        f.write(emit_token_print(kws, puncts))
    print('Wrote:')
    print(f'  {OUT_INC}  (keywords: {len(kws)})')
    print(f'  {OUT_PUNCTS} (puncts: {len(puncts)})')
    print(f'  {OUT_PRINT}')


if __name__ == '__main__':
    sys.exit(main())
