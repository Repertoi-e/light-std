#!/usr/bin/env python3
# Generates token_generated.inc with helper functions for tokenizer.
# Parses the token_type enum from lang.h to extract keywords and punctuators.

import os
import sys
import re
from collections import defaultdict

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
LANG_H = os.path.join(SCRIPT_DIR, 'lang.h')
OUT_TOKEN_GEN = os.path.join(SCRIPT_DIR, 'src', 'token', 'token_generated.inc')


def parse_token_enum(path):
    """Parse the token_type enum from lang.h and extract tokens with their string literals."""
    keywords = []
    punctuators = []
    token_strings = {}
    all_token_strings = {}
    
    with open(path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Find the enum token_type definition
    enum_match = re.search(r'enum token_type\s*\{(.*?)\};', content, re.DOTALL)
    if not enum_match:
        raise ValueError("Could not find token_type enum in lang.h")
    
    enum_content = enum_match.group(1)
    lines = enum_content.split('\n')
    
    for line in lines:
        original_line = line  # Store original line for comment parsing
        line = line.strip()
        if not line or line.startswith('//') or line.startswith('#'):
            continue
            
        if '=' in line:
            # Handle assignment lines carefully to preserve full value
            parts = line.split('=', 1)
            token_name = parts[0].strip()
            value_part = parts[1].strip()
            # Remove trailing comma if present
            if value_part.endswith(','):
                value_part = value_part[:-1].strip()

            all_token_strings[token_name] = value_part

            # Handle single character tokens like TOKEN_DOT = '.'
            if value_part.startswith("'") and value_part.endswith("'") and len(value_part) == 3:
                char = value_part[1]
                punctuators.append((char, token_name))
                token_strings[token_name] = char
            
            # Handle keywords - they start at 0x10000000
            elif token_name.startswith('TOKEN_KW_'):
                # Check if there's a comment with the actual keyword in the original line
                comment_match = re.search(r'//\s*(\S+)', original_line)
                if comment_match:
                    kw = comment_match.group(1)
                    keywords.append((kw, token_name))
                    token_strings[token_name] = kw
                else:
                    # Fallback: extract keyword from token name
                    kw = token_name[9:]  # Remove TOKEN_KW_ prefix
                    keywords.append((kw, token_name))
                    token_strings[token_name] = kw
            
            # Handle multi-character operators
            elif 'TKN2' in value_part or 'TKN3' in value_part:
                # Extract characters from TKN2('a', 'b') or TKN3('a', 'b', 'c')
                chars_match = re.findall(r"'(.)'", value_part)
                if chars_match:
                    op = ''.join(chars_match)
                    punctuators.append((op, token_name))
                    token_strings[token_name] = op
        else:
            # Handle tokens without explicit values
            original_line_for_comment = line  # Save original line for comment parsing
            
            # Remove trailing comma and comments for token name extraction
            if ',' in line:
                line = line.split(',')[0].strip()
            
            if line.startswith('TOKEN_'):
                token_name = line
                # Check if there's a comment with the actual keyword
                comment_match = re.search(r'//\s*(\S+)', original_line_for_comment)

                all_token_strings[token_name] = token_name  # Default to enum name
                
                if token_name.startswith('TOKEN_KW_'):
                    if comment_match:
                        kw = comment_match.group(1)
                        keywords.append((kw, token_name))
                        token_strings[token_name] = kw
                    else:
                        # Fallback: extract keyword from token name
                        kw = token_name[9:]  # Remove TOKEN_KW_ prefix
                        keywords.append((kw, token_name))
                        token_strings[token_name] = kw
    
    return keywords, punctuators, token_strings, all_token_strings


def c_string_literal(s: str) -> str:
    """Escape string for inclusion inside C double quotes."""
    out = '"'
    for ch in s:
        if ch == '"':
            out += '\\"'
        elif ch == '\\':
            out += '\\\\'
        elif ch == '\n':
            out += '\\n'
        elif ch == '\t':
            out += '\\t'
        else:
            out += ch
    out += '"'
    return out


def emit_token_to_string_gen(token_strings):
    """Generate the token_to_string_gen function."""
    lines = []
    lines.append('// Generated by gen_tokenizer.py. Do not edit by hand.')
    lines.append('const char* token_to_string_gen(token_type t) {')
    lines.append('  switch (t) {')
    
    for token_enum, text in sorted(token_strings.items()):
        if text:  # Only add non-empty strings
            lines.append(f'    case {token_enum}: return {c_string_literal(text)};')
    
    lines.append('    default: return "<unknown>";')
    lines.append('  }')
    lines.append('}')
    return '\n'.join(lines)

def emit_token_type_to_string(token_strings):
    """Generate the token_type_to_string function."""
    lines = []
    lines.append('// Generated by gen_tokenizer.py. Do not edit by hand.')
    lines.append('const char* token_type_to_string(token_type t) {')
    lines.append('  switch (t) {')
    for token_enum in sorted(token_strings.keys()):
        lines.append(f'    case {token_enum}: return "{token_enum}";')
    lines.append('    default: return "<unknown>";')
    lines.append('  }')
    lines.append('}')
    return '\n'.join(lines)

def emit_token_classify_identifier(keywords):
    """Generate a function to classify identifiers as keywords using memcmp."""
    lines = []
    lines.append('// Generated by gen_tokenizer.py. Do not edit by hand.')
    lines.append('// Classify identifier tokens as keywords using optimized memcmp')
    lines.append('token_type token_classify_identifier(const char* str, s64 length) {')
    lines.append('  switch (length) {')
    
    # Group keywords by length for efficient switching
    kw_by_length = defaultdict(list)
    for kw, token_name in keywords:
        if kw:
            kw_by_length[len(kw)].append((kw, token_name))
    
    for length in sorted(kw_by_length.keys()):
        lines.append(f'    case {length}:')
        for kw, token_name in kw_by_length[length]:
            lines.append(f'      if (memcmp(str, "{kw}", {length}) == 0) return {token_name};')
        lines.append('      break;')
    
    lines.append('    default: break;')
    lines.append('  }')
    lines.append('  return TOKEN_IDENTIFIER;')
    lines.append('}')
    lines.append('')
    return '\n'.join(lines)

def emit_token_switch(punctuators):
    """Generate a threaded jump table for tokenizing punctuators and operators."""
    lines = []
    lines.append('// Generated by gen_tokenizer.py. Do not edit by hand.')
    lines.append('// Threaded jump table for character-by-character matching')
    lines.append('// Assumes null-terminated string with 16 null bytes padding at end')
    lines.append('// Keywords are handled separately by token_classify_identifier')
    lines.append('// Takes advantage of token_type encoding where ASCII chars map directly to their values')
    lines.append('// Uses u32 comparisons for multi-character operators')
    lines.append('token token_switch(tokenizer ref tz) {')
    lines.append('  const char* s = tz.Current;')
    lines.append('  if (!s[0]) return {TOKEN_INVALID, s - tz.Start };')
    lines.append('')
    
    # Group punctuators by first character, but only for multi-character ones
    # Single character punctuators can be handled directly
    multi_char_groups = defaultdict(list)
    single_char_set = set()
    
    for punct, token_name in punctuators:
        if len(punct) == 1:
            single_char_set.add(punct[0])
        else:
            multi_char_groups[punct[0]].append((punct, token_name))
    
    # Sort multi-character groups by length desc for longest match
    for key in multi_char_groups:
        multi_char_groups[key].sort(key=lambda x: -len(x[0]))
    
    # Identify identifier start characters (ASCII letters and underscore)
    ident_start_chars = set()
    for i in range(ord('A'), ord('Z') + 1):
        ident_start_chars.add(chr(i))
    for i in range(ord('a'), ord('z') + 1):
        ident_start_chars.add(chr(i))
    ident_start_chars.add('_')

    # Number start characters (including signs)
    number_start_chars = set()
    for i in range(ord('0'), ord('9') + 1):
        number_start_chars.add(chr(i))
    # Note: + and - are handled specially in their multi-char labels to check for following digits

    # Characters that need special handling (multi-character operators)
    special_chars = set(multi_char_groups.keys())
    
    # Create jump table
    lines.append('  static const void* jump_table[256] = {')
    for i in range(256):
        ch = chr(i) if i < 128 else None
        if ch and ch in special_chars:
            lines.append(f'    [0x{i:02X}] = &&label_{ord(ch):02X}, // \'{ch}\' (multi-char)')
        elif ch and ch in ident_start_chars:
            lines.append(f'    [0x{i:02X}] = &&label_ident, // \'{ch}\'')
        elif ch and ch in single_char_set and ch not in special_chars:
            lines.append(f'    [0x{i:02X}] = &&label_single_char, // \'{ch}\'')
        elif ch and ch in number_start_chars:
            lines.append(f'    [0x{i:02X}] = &&label_number, // \'{ch}\'')
        else:
            lines.append(f'    [0x{i:02X}] = &&label_default,')
    lines.append('  };')
    lines.append('')
    lines.append('  goto *jump_table[(unsigned char)s[0]];')
    lines.append('')
    
    # Generate labels for multi-character operators only
    for ch in sorted(special_chars):
        ch_hex = f'{ord(ch):02X}'
        lines.append(f'label_{ch_hex}: // \'{ch}\' multi-character')
        
        # Group by length for more efficient checking
        by_length = defaultdict(list)
        for punct, token_name in multi_char_groups[ch]:
            by_length[len(punct)].append((punct, token_name))
        
        # Process longest first
        for length in sorted(by_length.keys(), reverse=True):
            if length == 2:
                lines.append('  {')
                for punct, token_name in by_length[length]:
                    # For 2-char punct like "==", we only need to check s[1] since s[0] is guaranteed by jump table
                    char1 = punct[1] if len(punct) > 1 else '\0'
                    u8_val = ord(char1)
                    lines.append(f'    if (s[1] == 0x{u8_val:02X}) {{ tz.Current += {length}; return {{ {token_name}, s - tz.Start }}; }} // "{punct}"')
                lines.append('  }')
            elif length == 3:
                lines.append('  {')
                for punct, token_name in by_length[length]:
                    # For 3-char punct like "...", we only need to check s[1] and s[2] since s[0] is guaranteed
                    char1 = punct[1] if len(punct) > 1 else '\0'
                    char2 = punct[2] if len(punct) > 2 else '\0'
                    u8_val1 = ord(char1)
                    u8_val2 = ord(char2)
                    lines.append(f'    if (s[1] == 0x{u8_val1:02X} && s[2] == 0x{u8_val2:02X}) {{ tz.Current += {length}; return {{ {token_name}, s - tz.Start }}; }} // "{punct}"')
                lines.append('  }')
            else:
                # Fallback for longer sequences (shouldn't happen with typical operators)
                for punct, token_name in by_length[length]:
                    conditions = []
                    for i, c in enumerate(punct[1:], 1):
                        conditions.append(f"s[{i}] == '{c}'")
                    if conditions:
                        cond_str = ' && '.join(conditions)
                        lines.append(f'  if ({cond_str}) {{ tz.Current += {length}; return {{ {token_name}, s - tz.Start }}; }} // "{punct}"')
        
        # If no multi-character match, fall through to single character
        lines.append(f'  // Fall through to single character: {ch}')
        
        # Special case for dot: check if followed by a digit to parse as float
        if ch == '.':
            lines.append('  // Special case: .123 should be parsed as float, not DOT + INTEGER')
            lines.append('  if (ascii_is_digit(s[1])) return tokenizer_next_number_literal(tz);')
        
        # Special case for + and -: check if followed by a digit to parse as signed number
        if ch == '+' or ch == '-':
            lines.append(f'  // Special case: {ch}123 should be parsed as signed number, not {ch.upper()} + INTEGER')
            lines.append('  if (ascii_is_digit(s[1])) return tokenizer_next_number_literal(tz);')
        
        lines.append('  tz.Current += 1;')
        lines.append(f'  return {{ (token_type)\'{ch}\', s - tz.Start }};')
        lines.append('')
    
    lines.append('label_single_char:')
    lines.append('  // Single character punctuator - token type is the ASCII value')
    lines.append('  tz.Current += 1;')
    lines.append('  return { (token_type)s[0], s - tz.Start };')
    lines.append('')

    lines.append('label_number:')
    lines.append('  // Handle number literals using separate parse_number function')
    lines.append('  return tokenizer_next_number_literal(tz);')
    lines.append('')

    lines.append('label_ident:')
    lines.append('  // Handle ASCII identifier characters')
    lines.append('  do {')
    lines.append('    s++;')
    lines.append('  } while (ascii_is_identifier_cont(*s));')
    lines.append('  tz.Current = s;')
    lines.append('  return { TOKEN_IDENTIFIER, s - tz.Start };')
    lines.append('')
    
    lines.append('label_default:')
    lines.append('  // Non-ASCII or invalid character')
    lines.append('  return { TOKEN_INVALID, s - tz.Start };')
    lines.append('}')
    
    return '\n'.join(lines)


def generate_token_inc(keywords, punctuators, token_strings, all_token_strings):
    """Generate the complete token_generated.inc file."""
    lines = []
    lines.append('/* ')
    lines.append(' * Generated by gen_tokenizer.py. Do not edit by hand.')
    lines.append(' * This file contains auto-generated token helper functions')
    lines.append(' * ')
    lines.append(f' * Keywords: {len(keywords)}')
    lines.append(f' * Punctuators: {len(punctuators)}')
    lines.append(f' * Total tokens: {len(token_strings)}')
    lines.append(' */')
    lines.append('')
    
    # Add the token to string function
    lines.append(emit_token_to_string_gen(token_strings))
    lines.append('')
    lines.append(emit_token_type_to_string(all_token_strings))
    lines.append('')
    
    # Add the identifier classification function
    lines.append(emit_token_classify_identifier(keywords))
    lines.append('')
    
    # Add the token switch function (punctuators only)
    lines.append(emit_token_switch(punctuators))
    
    return '\n'.join(lines)


def main():
    try:
        keywords, punctuators, token_strings, all_token_strings = parse_token_enum(LANG_H)
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(OUT_TOKEN_GEN), exist_ok=True)
        
        # Generate the token_generated.inc file
        token_inc_content = generate_token_inc(keywords, punctuators, token_strings, all_token_strings)
        
        with open(OUT_TOKEN_GEN, 'w', encoding='utf-8') as f:
            f.write(token_inc_content)
        
        print(f'Generated {OUT_TOKEN_GEN}')
        print(f'  Keywords: {len(keywords)}')
        print(f'  Punctuators: {len(punctuators)}')
        print(f'  Total tokens: {len(token_strings)}')
        
        return 0
        
    except Exception as e:
        print(f'Error: {e}', file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
